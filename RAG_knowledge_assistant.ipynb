{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a4e0e56",
   "metadata": {},
   "source": [
    "### Install the requaired library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3b2ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q aixplain faiss-cpu pypdf pypdf2 pdfplumber PyPDF2 scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81701b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q aixplain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163167cf",
   "metadata": {},
   "source": [
    "### Add Your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53fa02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your aiXplain API key\n",
    "os.environ[\"AIXPLAIN_API_KEY\"] = \"please replace_with_your_actual_api_key\"\n",
    "print(os.environ[\"AIXPLAIN_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347230a3",
   "metadata": {},
   "source": [
    "### Verify that the API key is working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35756abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixplain.factories import ModelFactory\n",
    "\n",
    "model = ModelFactory.get(\"673248d66eb563b2b00f75d1\")\n",
    "res = model.run(\"hello hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b8d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import faiss\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixplain.factories import IndexFactory\n",
    "from aixplain.modules.model.index_model import Splitter, IndexFilter, IndexFilterOperator\n",
    "from aixplain.modules.model.record import Record\n",
    "from aixplain.enums.splitting_options import SplittingOptions\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# PDF file path\n",
    "pdf_path = \"data/pdfs/Careers-and-Educational-Guidance-Policy-2025-26.pdf\"\n",
    "\n",
    "# Extract text from PDF\n",
    "reader = PdfReader(pdf_path)\n",
    "full_text = \"\"\n",
    "total_pages = len(reader.pages)\n",
    "\n",
    "for page_num, page in enumerate(reader.pages):\n",
    "    page_text = page.extract_text()\n",
    "    if page_text:\n",
    "        full_text += f\"[PAGE {page_num + 1}]\\n{page_text}\\n\"\n",
    "\n",
    "print(f\"âœ“ Extracted text from {total_pages} pages\")\n",
    "print(f\"âœ“ Total characters: {len(full_text):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d614ce0",
   "metadata": {},
   "source": [
    "Configure the text splitter and process the PDF documents. This prepares the data for the RAG index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced chunking with Splitter - optimized for policy documents\n",
    "splitter = Splitter(\n",
    "    split=True,\n",
    "    split_by=SplittingOptions.SENTENCE,\n",
    "    split_length=10,      # 10 sentences per chunk (paragraph-level)\n",
    "    split_overlap=2       # 2 sentence overlap for context continuity\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Splitter configured for sentence-based chunking\")\n",
    "print(f\"  - Split method: SENTENCE\")\n",
    "print(f\"  - Chunk size: 10 sentences\")\n",
    "print(f\"  - Overlap: 2 sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f054347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index with metadata for education policy\n",
    "try:\n",
    "    index = IndexFactory.create(\n",
    "        name=\"Education Guidance Policy Index\",\n",
    "        description=(\n",
    "            \"Comprehensive educational guidance and policy document \"\n",
    "            \"containing federal requirements, guidelines, and best practices \"\n",
    "            \"for educational agencies and institutions.\"\n",
    "        ),\n",
    "        embedding_model=\"678a4f8547f687504744960a\"  # Snowflake Arctic\n",
    "\n",
    "    )\n",
    "    print(f\"âœ“ Index created! ID: {index.id}\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e):\n",
    "        # If index already exists, retrieve it by known ID\n",
    "        try:\n",
    "            index = IndexFactory.get(\"694285b39dcf6413b67dd5fb\")\n",
    "            print(f\"âœ“ Using existing index! ID: {index.id}\")\n",
    "        except Exception as fetch_error:\n",
    "            print(f\"âœ— Failed to retrieve existing index: {fetch_error}\")\n",
    "            raise\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "    print(f\"Index created with ID: {index.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create records with rich metadata for filtering and citations\n",
    "records = []\n",
    "\n",
    "# Extract metadata from PDF\n",
    "pdf_filename = os.path.basename(pdf_path)\n",
    "upload_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "for i, chunk_text in enumerate(full_text.split(\"\\n\\n\")[:50]):  # Limit for demo\n",
    "    if not chunk_text.strip() or len(chunk_text.strip()) < 100:\n",
    "        continue\n",
    "    \n",
    "    # Extract page number if available\n",
    "    page_match = chunk_text.split(\"[PAGE\")[0]\n",
    "    page_num = i // 5 + 1  # Approximate page number\n",
    "    \n",
    "    record = Record(\n",
    "        id=f\"chunk_{i}\",\n",
    "        value=chunk_text.strip(),\n",
    "        value_type=\"text\",\n",
    "        attributes={\n",
    "            # Source attribution\n",
    "            \"source_title\": \"Careers and Educational Guidance Policy 2025-26\",\n",
    "            \"source_filename\": pdf_filename,\n",
    "            \"source_url\": \"internal://education-policy\",\n",
    "            \n",
    "            # Organization\n",
    "            \"doc_type\": \"policy\",\n",
    "            \"category\": \"education_guidance\",\n",
    "            \"section\": \"policy\",\n",
    "            \n",
    "            # Tracking\n",
    "            \"chunk_id\": i,\n",
    "            \"page_number\": page_num,\n",
    "            \"upload_date\": upload_date,\n",
    "            \"last_updated\": upload_date,\n",
    "            \n",
    "            # Searchability\n",
    "            \"priority\": \"high\",\n",
    "            \"tags\": [\"education\", \"guidance\", \"policy\", \"federal\", \"career\"]\n",
    "        }\n",
    "    )\n",
    "    records.append(record)\n",
    "\n",
    "print(f\"âœ“ Created {len(records)} enriched records with metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa3b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload records with intelligent chunking and error handling\n",
    "batch_size = 5\n",
    "successful_uploads = 0\n",
    "failed_batches = []\n",
    "\n",
    "for i in range(0, len(records), batch_size):\n",
    "    batch_number = (i // batch_size) + 1\n",
    "    batch = records[i:i + batch_size]\n",
    "\n",
    "    try:\n",
    "        # Upsert with splitter for automatic chunking\n",
    "        index.upsert(batch, splitter=splitter)\n",
    "        successful_uploads += len(batch)\n",
    "        print(\n",
    "            f\"âœ“ Batch {batch_number}: Uploaded {len(batch)} records \"\n",
    "            f\"(Total: {successful_uploads}/{len(records)})\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        failed_batches.append((batch_number, str(e)))\n",
    "        print(f\"âœ— Batch {batch_number} failed: {e}\")\n",
    "\n",
    "print(f\"\\nâœ“ Upload complete: {successful_uploads} records indexed successfully\")\n",
    "if failed_batches:\n",
    "    print(f\"âš  {len(failed_batches)} batches failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e240b39",
   "metadata": {},
   "source": [
    "## Knowledge Base Statistics & Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d698df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify index and retrieve statistics\n",
    "doc_count = index.count()\n",
    "print(f\"Total documents in index: {doc_count}\")\n",
    "print(f\"Index ID: {index.id}\")\n",
    "print(f\"Upload timestamp: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299dbc06",
   "metadata": {},
   "source": [
    "## Advanced Search with Filtering & Citations\n",
    "\n",
    "The search system now supports:\n",
    "- **Semantic search**: Find documents by meaning, not just keywords\n",
    "- **Metadata filtering**: Filter by category, priority, tags, dates\n",
    "- **Source citations**: All results include source attribution\n",
    "- **Relevance scoring**: See similarity scores (0-1 scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4dc830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic semantic search with citations\n",
    "print(\"=\" * 70)\n",
    "print(\"SEARCH EXAMPLE 1: Basic Query with Citations\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "response = index.search(\n",
    "    \"federal requirements for educational guidance policies\",\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "print(f\"\\nFound {len(response.details)} results:\\n\")\n",
    "for i, result in enumerate(response.details, 1):\n",
    "    metadata = result.get('metadata', {})\n",
    "    source_title = metadata.get('source_title', 'Unknown')\n",
    "    last_updated = metadata.get('last_updated', 'N/A')\n",
    "    score = result['score']\n",
    "    \n",
    "    print(f\"{i}. Relevance Score: {score:.1%}\") \n",
    "    print(f\"   Content: {result['data'][:120]}...\")\n",
    "    print(f\"   ðŸ“š Source: {source_title}\")\n",
    "    print(f\"   ðŸ“… Updated: {last_updated}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa6cc3",
   "metadata": {},
   "source": [
    "## Advanced Filtered Search\n",
    "\n",
    "Filter searches by category, priority, tags, and date ranges for precise results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7462804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Filtered search - High priority education content\n",
    "print(\"=\" * 70)\n",
    "print(\"SEARCH EXAMPLE 2: Filtered by Priority\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "priority_filter = IndexFilter(\n",
    "    field=\"priority\",\n",
    "    value=\"high\",\n",
    "    operator=IndexFilterOperator.EQUALS\n",
    ")\n",
    "\n",
    "filtered_response = index.search(\n",
    "    \"student guidance and career pathways\",\n",
    "    filters=[priority_filter],\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "print(f\"\\nHigh-priority results about guidance and career:\\n\")\n",
    "for i, result in enumerate(filtered_response.details, 1):\n",
    "    metadata = result.get('metadata', {})\n",
    "    tags = metadata.get('tags', [])\n",
    "    \n",
    "    print(f\"{i}. Score: {result['score']:.1%}\")\n",
    "    print(f\"   Category: {metadata.get('category', 'N/A')}\")\n",
    "    print(f\"   Tags: {', '.join(tags) if tags else 'None'}\")\n",
    "    print(f\"   Content: {result['data'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d914b",
   "metadata": {},
   "source": [
    "## SQL Tool\n",
    "\n",
    "### Interact with SQLite databases and CSV files, create and manage tables, execute read/write queries, and print formatted output.\n",
    "\n",
    "### Key Features\n",
    "- Automatic CSV-to-SQLite conversion\n",
    "- Schema inference and validation\n",
    "- Column name cleaning for SQLite compatibility\n",
    "- Support for both read-only and write operations\n",
    "- Comprehensive error handling and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43954906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixplain.modules.agent.output_format import OutputFormat\n",
    "from aixplain.modules.agent.tool.sql_tool import SQLTool\n",
    "\n",
    "# Create a SQL tool that works with a CSV\n",
    "sql_tool = SQLTool(\n",
    "    name=\"ESEA Report Card Analyzer\",\n",
    "    description=\"OPPORTUNITIES AND RESPONSIBILITIES FOR STATE AND LOCAL REPORT CARDS U.S. Department of Education Under the Elementary and Secondary Education Act of 1965\",\n",
    "    database=\"data/ESEA_Report_Card_Guidelines.db\",  # database file\n",
    "    source_type=\"csv\",                 \n",
    "    enable_commit=False               \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070575ab",
   "metadata": {},
   "source": [
    "## Education Policy Website Scraper\n",
    "\n",
    "- Scrapes predefined landing pages or selected subpages on ed.gov\n",
    "\n",
    "- Limits output to first 15 paragraphs per page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4157d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_scraper(url: str, max_paragraphs: int = 15, follow_links: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Education Policy Scraper (1-Level Links)\n",
    "    - Scrapes key policy text from a predefined education website\n",
    "    - Follows first-level internal links to get full articles\n",
    "    - Limits output to `max_paragraphs` per page for efficiency\n",
    "    - Input: URL of the page to scrape\n",
    "    - Output: Concatenated text from page and linked articles\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.system(\"pip install -q requests beautifulsoup4 2>/dev/null\")\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    from urllib.parse import urljoin, urlparse\n",
    "\n",
    "    # Define headers to mimic a browser and avoid 403 Forbidden errors\n",
    "    HEADERS = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    def scrape_page(page_url: str, paragraphs: int) -> str:\n",
    "        try:\n",
    "            print(f\"[DEBUG] Scraping: {page_url}\")\n",
    "            response = requests.get(page_url, headers=HEADERS, timeout=20)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            # Strategy 1: Look for main content area to avoid nav/footer noise\n",
    "            main_content = soup.find(\"main\") or soup.find(\"article\") or soup.find(\"div\", role=\"main\") or soup.find(\"div\", id=\"content\")\n",
    "            \n",
    "            if main_content:\n",
    "                search_area = main_content\n",
    "            else:\n",
    "                search_area = soup.body or soup\n",
    "\n",
    "            # Strategy 2: Extract text from paragraphs and list items\n",
    "            text_elements = []\n",
    "            \n",
    "            # Get paragraphs\n",
    "            text_elements.extend(search_area.find_all(\"p\"))\n",
    "            \n",
    "            # Get list items (often used for policy points)\n",
    "            text_elements.extend(search_area.find_all(\"li\"))\n",
    "            \n",
    "            # Get divs with specific classes if no paragraphs found\n",
    "            if not text_elements:\n",
    "                 text_elements.extend(search_area.find_all(\"div\", class_=[\"content\", \"field-item\", \"body\"]))\n",
    "\n",
    "            # Filter and clean text\n",
    "            cleaned_texts = []\n",
    "            for el in text_elements:\n",
    "                text = el.get_text(strip=True)\n",
    "                # Filter out short navigation items or empty strings\n",
    "                if len(text) > 30: \n",
    "                    cleaned_texts.append(text)\n",
    "            \n",
    "            # Limit to requested number of paragraphs/items\n",
    "            final_text = \"\\n\\n\".join(cleaned_texts[:paragraphs])\n",
    "            \n",
    "            print(f\"[DEBUG] Found {len(cleaned_texts)} text blocks, {len(final_text)} characters\")\n",
    "            return final_text if final_text else f\"Could not extract text from {page_url}\"\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error fetching {page_url}: {str(e)}\"\n",
    "            print(f\"[DEBUG] {error_msg}\")\n",
    "            return error_msg\n",
    "\n",
    "    print(f\"[DEBUG] Starting scraper for: {url}\")\n",
    "    content = scrape_page(url, max_paragraphs)\n",
    "\n",
    "    if follow_links and content and \"Error\" not in content:\n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=20)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Find all internal links (same domain)\n",
    "            base_domain = urlparse(url).netloc\n",
    "            internal_links = []\n",
    "            \n",
    "            # Search in main content if possible\n",
    "            main_content = soup.find(\"main\") or soup.find(\"article\") or soup.body\n",
    "            \n",
    "            if main_content:\n",
    "                for a in main_content.find_all(\"a\", href=True):\n",
    "                    link = urljoin(url, a['href'])\n",
    "                    if urlparse(link).netloc == base_domain and link != url:\n",
    "                        internal_links.append(link)\n",
    "\n",
    "            print(f\"[DEBUG] Found {len(internal_links)} internal links\")\n",
    "            \n",
    "            # Scrape first 2 internal links only for efficiency\n",
    "            for i, link in enumerate(internal_links[:2]):\n",
    "                print(f\"[DEBUG] Scraping linked page {i+1}: {link}\")\n",
    "                content += \"\\n\\n---LINKED CONTENT---\\n\\n\" + scrape_page(link, max_paragraphs)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[DEBUG] Error following links: {str(e)}\")\n",
    "            content += f\"\\n\\nError following links: {str(e)}\"\n",
    "\n",
    "    return content if content else \"No text found on the page.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a731fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST THE SCRAPER LOCALLY\n",
    "\n",
    "test_url = \"https://www.ed.gov/laws-and-policy/laws-preschool-grade-12-education/esea/what-is-the-every-student-succeeds-act\"\n",
    "print(f\"Testing scraper on: {test_url}\\n\")\n",
    "\n",
    "# Call the function directly (not through the agent) to see DEBUG logs\n",
    "result = policy_scraper(test_url, max_paragraphs=5, follow_links=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EXTRACTED CONTENT:\")\n",
    "print(\"=\"*50)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6f705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixplain.factories import ModelFactory\n",
    "\n",
    "scraper_tool = ModelFactory.create_utility_model(\n",
    "    name=\"Education Policy Scraper\",\n",
    "    description=(\n",
    "        \"Scrapes key policy text from education.gov pages. \"\n",
    "        \"Follows first-level internal links to include full articles. \"\n",
    "        \"Limits to 10 paragraphs per page to save resources.\"\n",
    "    ),\n",
    "    code=policy_scraper\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf6afc",
   "metadata": {},
   "source": [
    "tools allow the agent to Save, List, and Read notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_note(content: str, topic: str = \"General\") -> str:\n",
    "    \"\"\"\n",
    "    Saves a summary note or key information to a local file for the user.\n",
    "    Use this when the user asks to \"save this\", \"remind me\", \"create a note\", or \"keep this information\".\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Create notes directory if it doesn't exist\n",
    "    os.makedirs(\"user_notes\", exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    filename = f\"user_notes/note_{timestamp}.txt\"\n",
    "    \n",
    "    note_content = f\"TOPIC: {topic}\\nDATE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n{content}\\n\"\n",
    "    \n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(note_content)\n",
    "        return f\"Successfully saved note to {filename}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error saving note: {str(e)}\"\n",
    "\n",
    "note_saver_tool = ModelFactory.create_utility_model(\n",
    "    name=\"Note Saver\",\n",
    "    description=\"Saves text content to a local file. Use when user wants to save a summary, reminder, or specific information.\",\n",
    "    code=save_note\n",
    ")\n",
    "print(f\"âœ“ Note Saver tool created with ID: {note_saver_tool.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a734bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_notes(query: str = \"all\") -> str:\n",
    "    \"\"\"\n",
    "    Lists all saved notes in the user_notes directory.\n",
    "    Use this when the user asks to \"see my notes\", \"list notes\", \"show my reminders\", or \"what have I saved\".\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Create directory if it doesn't exist, so we don't return an error\n",
    "    if not os.path.exists(\"user_notes\"):\n",
    "        os.makedirs(\"user_notes\", exist_ok=True)\n",
    "        return \"No notes found (Directory was empty).\"\n",
    "    \n",
    "    files = os.listdir(\"user_notes\")\n",
    "    if not files:\n",
    "        return \"No notes found.\"\n",
    "    \n",
    "    # Sort by time (newest first)\n",
    "    files.sort(reverse=True)\n",
    "    \n",
    "    file_list = \"\\n\".join([f\"- {f}\" for f in files])\n",
    "    return f\"Found the following notes (Copy the exact filename to read one):\\n{file_list}\"\n",
    "\n",
    "def read_note(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads the content of a specific note file.\n",
    "    Use this when the user asks to \"read note X\".\n",
    "    IMPORTANT: The filename must be EXACTLY as shown in the list_notes output.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Clean filename\n",
    "    filename = filename.strip()\n",
    "    \n",
    "    # Security check\n",
    "    if \"..\" in filename or \"/\" in filename:\n",
    "        return \"Error: Invalid filename. Do not use paths.\"\n",
    "        \n",
    "    filepath = f\"user_notes/{filename}\"\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        return f\"STOP: Note '{filename}' not found. Please ask the user to list notes first to get the correct name.\"\n",
    "        \n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return f\"Error reading note: {str(e)}\"\n",
    "\n",
    "list_notes_tool = ModelFactory.create_utility_model(\n",
    "    name=\"List Notes\",\n",
    "    description=\"Lists all files in the user_notes directory.\",\n",
    "    code=list_notes\n",
    ")\n",
    "\n",
    "read_note_tool = ModelFactory.create_utility_model(\n",
    "    name=\"Read Note\",\n",
    "    description=\"Reads the content of a specific note file.\",\n",
    "    code=read_note\n",
    ")\n",
    "\n",
    "print(f\"âœ“ List Notes tool created with ID: {list_notes_tool.id}\")\n",
    "print(f\"âœ“ Read Note tool created with ID: {read_note_tool.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3847460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all tool IDs for use in app.py\n",
    "print(\"=\" * 70)\n",
    "print(\"TOOL IDs FOR STREAMLIT APP\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"SCRAPER_TOOL_ID = \\\"{scraper_tool.id}\\\"\")\n",
    "print(f\"INDEX_TOOL_ID = \\\"{index.id}\\\"\")\n",
    "# print(f\"SQL_TOOL_ID = \\\"{sql_tool.id}\\\"\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nCopy these IDs into your app.py file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46be40d1",
   "metadata": {},
   "source": [
    "## Creating a knowledge_assistant Agent\n",
    "Uses both pdf tool and the SQL tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37562fd",
   "metadata": {},
   "source": [
    "Here we assemble the agent. We define its persona, instructions, and give it access to all the tools we created (Index, SQL, Scraper, Notes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15101f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixplain.factories import AgentFactory\n",
    "\n",
    "# Create production-ready RAG agent with citation support\n",
    "knowledge_assistant = AgentFactory.create(\n",
    "    name=\"Education Policy Advisor\",\n",
    "    description=(\n",
    "        \"Education Policy Advisor is an AI agent specialized in providing authoritative \"\n",
    "        \"guidance on education policies and federal education requirements. \"\n",
    "        \"It retrieves information from PDF Index, ESEA Report Card database, and Web scraping. \"\n",
    "        \"It can also save, list, and read notes for the user upon request. \"\n",
    "    ),\n",
    "    llm_id=\"669a63646eb56306647e1091\",  # GPT 4o mini\n",
    "    instructions=\"\"\"\n",
    "**MISSION STATEMENT**\n",
    "You are an Education Policy Advisor. Your goal is to provide helpful, accurate information about education policies, federal requirements, and guidance. You should prioritize information from your provided tools (PDF Index, SQL Database, Web Scraper, Notes).\n",
    "\n",
    "**OPERATIONAL GUIDELINES**\n",
    "\n",
    "**1. Source Usage**\n",
    "- **Primary Sources:** Always check your tools (PDF, SQL, Scraper) first for specific policy details.\n",
    "- **Context:** You may use your general knowledge to explain terms, provide background context, or summarize findings to make the answer more helpful.\n",
    "- **Synthesis:** You are encouraged to synthesize information from multiple tools if needed to answer a complex question.\n",
    "- **URL Constraint:** If a URL is provided in the user query, you must ONLY provide information derived from that URL using the Web Scraper Tool. Do not include information from other sources unless explicitly asked.\n",
    "\n",
    "**2. Tool Selection Strategy**\n",
    "- **URL Provided:** Use the **Web Scraper Tool**.\n",
    "- **Cambridge/CSVPA/General Policy:** Start with the **PDF Index Tool**.\n",
    "- **ESEA/Federal Requirements:** Start with the **SQL Database Tool**.\n",
    "- **Notes:** Use **Note Saver**, **List Notes**, or **Read Note** as requested.\n",
    "- **Fallback:** If your first choice tool doesn't yield results, **please try other relevant tools** before giving up. Do not restrict yourself to just one source if the answer might be elsewhere.\n",
    "\n",
    "**3. Handling \"Not Found\"**\n",
    "- If you cannot find the information in any tool, state clearly: \"I couldn't find specific details in the provided documents.\"\n",
    "- You may then offer general information based on your training if it helps the user, but clearly distinguish it from the official policy documents.\n",
    "\n",
    "**RESPONSE FORMATTING**\n",
    "\n",
    "Please structure your responses clearly:\n",
    "### [Heading]\n",
    "[Content]\n",
    "- Bullet points for lists\n",
    "\n",
    "#### Sources\n",
    "- [List the tools or documents used]\n",
    "\n",
    "**SCOPE**\n",
    "- Education policies, guidance, and requirements.\n",
    "- Managing user notes.\n",
    "\n",
    "**TONE**\n",
    "- Professional, helpful, and direct.\n",
    "\"\"\",\n",
    "    tools=[sql_tool, scraper_tool, note_saver_tool, list_notes_tool, read_note_tool, AgentFactory.create_model_tool(\"694285b39dcf6413b67dd5fb\")]\n",
    ")\n",
    "\n",
    "print(\"âœ“ Production agent created with updated instructions\")\n",
    "try:\n",
    "    knowledge_assistant.deploy()\n",
    "    print(f\"âœ“ Agent deployed successfully with ID: {knowledge_assistant.id}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  Agent deployment skipped: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a70eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_assistant.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d972af2f",
   "metadata": {},
   "source": [
    "## Query the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d37c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = knowledge_assistant.run(\n",
    "    query=\"What are the responsibilities of an SEA and an LEA for preparing a report card?\",\n",
    "    output_format=OutputFormat.MARKDOWN\n",
    ")\n",
    "print(response.data.output)\n",
    "\n",
    "# Extract tool usage\n",
    "intermediate_steps = getattr(response.data, \"intermediate_steps\", None)\n",
    "if intermediate_steps:\n",
    "    print(\"\\n[Search Details]\")\n",
    "    for step in intermediate_steps:\n",
    "        tool_steps = step.get(\"tool_steps\")\n",
    "        if tool_steps:\n",
    "            for tool_step in tool_steps:\n",
    "                print(f\"- Query: {tool_step.get('input', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c2619d",
   "metadata": {},
   "source": [
    "## Agent Testing PDF Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3bf47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = knowledge_assistant.run(\n",
    "    query=\"What are the federal requirements for educational guidance programs?\",\n",
    "    output_format=OutputFormat.MARKDOWN\n",
    ")\n",
    "print(response.data.output)\n",
    "\n",
    "# Extract tool usage\n",
    "intermediate_steps = getattr(response.data, \"intermediate_steps\", None)\n",
    "if intermediate_steps:\n",
    "    print(\"\\n[Search Details]\")\n",
    "    for step in intermediate_steps:\n",
    "        tool_steps = step.get(\"tool_steps\")\n",
    "        if tool_steps:\n",
    "            for tool_step in tool_steps:\n",
    "                print(f\"- Query: {tool_step.get('input', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4496b9",
   "metadata": {},
   "source": [
    "### Testing the Scrapping Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5687ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = knowledge_assistant.run(\n",
    "    query= \"According to this URL What is the Every Student Succeeds Act?: https://www.ed.gov/laws-and-policy/laws-preschool-grade-12-education/esea/what-is-the-every-student-succeeds-act \",\n",
    "    output_format=OutputFormat.MARKDOWN\n",
    ")\n",
    "print(response.data.output)\n",
    "\n",
    "# Extract tool usage\n",
    "intermediate_steps = getattr(response.data, \"intermediate_steps\", None)\n",
    "if intermediate_steps:\n",
    "    print(\"\\n[Search Details]\")\n",
    "    for step in intermediate_steps:\n",
    "        tool_steps = step.get(\"tool_steps\")\n",
    "        if tool_steps:\n",
    "            for tool_step in tool_steps:\n",
    "                print(f\"- Query: {tool_step.get('input', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d14cf00",
   "metadata": {},
   "source": [
    "## Agent Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c95b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    deployment = knowledge_assistant.deploy()\n",
    "    print(f\"knowledge_assistant deployed successfully with ID: {knowledge_assistant.id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Deployment failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ebb119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOMATICALLY UPDATE CONFIG.PY\n",
    "# Run this cell to update your backend configuration with the new Agent ID\n",
    "def update_config_file(new_agent_id):\n",
    "    config_path = \"config.py\"\n",
    "    try:\n",
    "        with open(config_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        with open(config_path, \"w\") as f:\n",
    "            for line in lines:\n",
    "                if line.startswith(\"AGENT_ID =\"):\n",
    "                    f.write(f'AGENT_ID = \"{new_agent_id}\"\\n')\n",
    "                else:\n",
    "                    f.write(line)\n",
    "        print(f\"âœ“ Successfully updated config.py with Agent ID: {new_agent_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Failed to update config.py: {e}\")\n",
    "\n",
    "if 'knowledge_assistant' in locals():\n",
    "    update_config_file(knowledge_assistant.id)\n",
    "else:\n",
    "    print(\"âš  Agent 'knowledge_assistant' not found. Did you run the agent creation cell?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
