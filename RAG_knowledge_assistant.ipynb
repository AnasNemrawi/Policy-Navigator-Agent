{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a4e0e56",
   "metadata": {},
   "source": [
    "### Install the requaired library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3b2ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q aixplain faiss-cpu pypdf pypdf2 pdfplumber PyPDF2 scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81701b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q aixplain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163167cf",
   "metadata": {},
   "source": [
    "### Add Your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53fa02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your aiXplain API key\n",
    "os.environ[\"AIXPLAIN_API_KEY\"] = \"Add you api key here\"\n",
    "print(os.environ[\"AIXPLAIN_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347230a3",
   "metadata": {},
   "source": [
    "### Verify that the API key is working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35756abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixplain.factories import ModelFactory\n",
    "\n",
    "model = ModelFactory.get(\"673248d66eb563b2b00f75d1\")\n",
    "res = model.run(\"hello hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b8d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import faiss\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixplain.factories import IndexFactory\n",
    "from aixplain.modules.model.index_model import Splitter, IndexFilter, IndexFilterOperator\n",
    "from aixplain.modules.model.record import Record\n",
    "from aixplain.enums.splitting_options import SplittingOptions\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# PDF file path\n",
    "pdf_path = \"data/pdfs/Careers-and-Educational-Guidance-Policy-2025-26.pdf\"\n",
    "\n",
    "# Extract text from PDF\n",
    "reader = PdfReader(pdf_path)\n",
    "full_text = \"\"\n",
    "total_pages = len(reader.pages)\n",
    "\n",
    "for page_num, page in enumerate(reader.pages):\n",
    "    page_text = page.extract_text()\n",
    "    if page_text:\n",
    "        full_text += f\"[PAGE {page_num + 1}]\\n{page_text}\\n\"\n",
    "\n",
    "print(f\"âœ“ Extracted text from {total_pages} pages\")\n",
    "print(f\"âœ“ Total characters: {len(full_text):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced chunking with Splitter - optimized for policy documents\n",
    "splitter = Splitter(\n",
    "    split=True,\n",
    "    split_by=SplittingOptions.SENTENCE,\n",
    "    split_length=10,      # 10 sentences per chunk (paragraph-level)\n",
    "    split_overlap=2       # 2 sentence overlap for context continuity\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Splitter configured for sentence-based chunking\")\n",
    "print(f\"  - Split method: SENTENCE\")\n",
    "print(f\"  - Chunk size: 10 sentences\")\n",
    "print(f\"  - Overlap: 2 sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f054347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index with metadata for education policy\n",
    "try:\n",
    "    index = IndexFactory.create(\n",
    "        name=\"Education Guidance Policy Index\",\n",
    "        description=(\n",
    "            \"Comprehensive educational guidance and policy document \"\n",
    "            \"containing federal requirements, guidelines, and best practices \"\n",
    "            \"for educational agencies and institutions.\"\n",
    "        ),\n",
    "        embedding_model=\"678a4f8547f687504744960a\"  # Snowflake Arctic\n",
    "\n",
    "    )\n",
    "    print(f\"âœ“ Index created! ID: {index.id}\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e):\n",
    "        # If index already exists, retrieve it by known ID\n",
    "        try:\n",
    "            index = IndexFactory.get(\"694285b39dcf6413b67dd5fb\")\n",
    "            print(f\"âœ“ Using existing index! ID: {index.id}\")\n",
    "        except Exception as fetch_error:\n",
    "            print(f\"âœ— Failed to retrieve existing index: {fetch_error}\")\n",
    "            raise\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "    print(f\"Index created with ID: {index.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create records with rich metadata for filtering and citations\n",
    "records = []\n",
    "\n",
    "# Extract metadata from PDF\n",
    "pdf_filename = os.path.basename(pdf_path)\n",
    "upload_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "for i, chunk_text in enumerate(full_text.split(\"\\n\\n\")[:50]):  # Limit for demo\n",
    "    if not chunk_text.strip() or len(chunk_text.strip()) < 100:\n",
    "        continue\n",
    "    \n",
    "    # Extract page number if available\n",
    "    page_match = chunk_text.split(\"[PAGE\")[0]\n",
    "    page_num = i // 5 + 1  # Approximate page number\n",
    "    \n",
    "    record = Record(\n",
    "        id=f\"chunk_{i}\",\n",
    "        value=chunk_text.strip(),\n",
    "        value_type=\"text\",\n",
    "        attributes={\n",
    "            # Source attribution\n",
    "            \"source_title\": \"Careers and Educational Guidance Policy 2025-26\",\n",
    "            \"source_filename\": pdf_filename,\n",
    "            \"source_url\": \"internal://education-policy\",\n",
    "            \n",
    "            # Organization\n",
    "            \"doc_type\": \"policy\",\n",
    "            \"category\": \"education_guidance\",\n",
    "            \"section\": \"policy\",\n",
    "            \n",
    "            # Tracking\n",
    "            \"chunk_id\": i,\n",
    "            \"page_number\": page_num,\n",
    "            \"upload_date\": upload_date,\n",
    "            \"last_updated\": upload_date,\n",
    "            \n",
    "            # Searchability\n",
    "            \"priority\": \"high\",\n",
    "            \"tags\": [\"education\", \"guidance\", \"policy\", \"federal\", \"career\"]\n",
    "        }\n",
    "    )\n",
    "    records.append(record)\n",
    "\n",
    "print(f\"âœ“ Created {len(records)} enriched records with metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa3b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload records with intelligent chunking and error handling\n",
    "batch_size = 5\n",
    "successful_uploads = 0\n",
    "failed_batches = []\n",
    "\n",
    "for i in range(0, len(records), batch_size):\n",
    "    batch_number = (i // batch_size) + 1\n",
    "    batch = records[i:i + batch_size]\n",
    "\n",
    "    try:\n",
    "        # Upsert with splitter for automatic chunking\n",
    "        index.upsert(batch, splitter=splitter)\n",
    "        successful_uploads += len(batch)\n",
    "        print(\n",
    "            f\"âœ“ Batch {batch_number}: Uploaded {len(batch)} records \"\n",
    "            f\"(Total: {successful_uploads}/{len(records)})\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        failed_batches.append((batch_number, str(e)))\n",
    "        print(f\"âœ— Batch {batch_number} failed: {e}\")\n",
    "\n",
    "print(f\"\\nâœ“ Upload complete: {successful_uploads} records indexed successfully\")\n",
    "if failed_batches:\n",
    "    print(f\"âš  {len(failed_batches)} batches failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e240b39",
   "metadata": {},
   "source": [
    "## Knowledge Base Statistics & Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d698df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify index and retrieve statistics\n",
    "doc_count = index.count()\n",
    "print(f\"Total documents in index: {doc_count}\")\n",
    "print(f\"Index ID: {index.id}\")\n",
    "print(f\"Upload timestamp: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299dbc06",
   "metadata": {},
   "source": [
    "## Advanced Search with Filtering & Citations\n",
    "\n",
    "The search system now supports:\n",
    "- **Semantic search**: Find documents by meaning, not just keywords\n",
    "- **Metadata filtering**: Filter by category, priority, tags, dates\n",
    "- **Source citations**: All results include source attribution\n",
    "- **Relevance scoring**: See similarity scores (0-1 scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4dc830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic semantic search with citations\n",
    "print(\"=\" * 70)\n",
    "print(\"SEARCH EXAMPLE 1: Basic Query with Citations\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "response = index.search(\n",
    "    \"federal requirements for educational guidance policies\",\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "print(f\"\\nFound {len(response.details)} results:\\n\")\n",
    "for i, result in enumerate(response.details, 1):\n",
    "    metadata = result.get('metadata', {})\n",
    "    source_title = metadata.get('source_title', 'Unknown')\n",
    "    last_updated = metadata.get('last_updated', 'N/A')\n",
    "    score = result['score']\n",
    "    \n",
    "    print(f\"{i}. Relevance Score: {score:.1%}\")\n",
    "    print(f\"   Content: {result['data'][:120]}...\")\n",
    "    print(f\"   ðŸ“š Source: {source_title}\")\n",
    "    print(f\"   ðŸ“… Updated: {last_updated}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa6cc3",
   "metadata": {},
   "source": [
    "## Advanced Filtered Search\n",
    "\n",
    "Filter searches by category, priority, tags, and date ranges for precise results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7462804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Filtered search - High priority education content\n",
    "print(\"=\" * 70)\n",
    "print(\"SEARCH EXAMPLE 2: Filtered by Priority\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "priority_filter = IndexFilter(\n",
    "    field=\"priority\",\n",
    "    value=\"high\",\n",
    "    operator=IndexFilterOperator.EQUALS\n",
    ")\n",
    "\n",
    "filtered_response = index.search(\n",
    "    \"student guidance and career pathways\",\n",
    "    filters=[priority_filter],\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "print(f\"\\nHigh-priority results about guidance and career:\\n\")\n",
    "for i, result in enumerate(filtered_response.details, 1):\n",
    "    metadata = result.get('metadata', {})\n",
    "    tags = metadata.get('tags', [])\n",
    "    \n",
    "    print(f\"{i}. Score: {result['score']:.1%}\")\n",
    "    print(f\"   Category: {metadata.get('category', 'N/A')}\")\n",
    "    print(f\"   Tags: {', '.join(tags) if tags else 'None'}\")\n",
    "    print(f\"   Content: {result['data'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d914b",
   "metadata": {},
   "source": [
    "## SQL Tool\n",
    "\n",
    "### Interact with SQLite databases and CSV files, create and manage tables, execute read/write queries, and print formatted output.\n",
    "\n",
    "### Key Features\n",
    "- Automatic CSV-to-SQLite conversion\n",
    "- Schema inference and validation\n",
    "- Column name cleaning for SQLite compatibility\n",
    "- Support for both read-only and write operations\n",
    "- Comprehensive error handling and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43954906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixplain.modules.agent.output_format import OutputFormat\n",
    "from aixplain.modules.agent.tool.sql_tool import SQLTool\n",
    "\n",
    "# Create a SQL tool that works with a CSV\n",
    "sql_tool = SQLTool(\n",
    "    name=\"ESEA Report Card Analyzer\",\n",
    "    description=\"OPPORTUNITIES AND RESPONSIBILITIES FOR STATE AND LOCAL REPORT CARDS U.S. Department of Education Under the Elementary and Secondary Education Act of 1965\",\n",
    "    database=\"data/ESEA_Report_Card_Guidelines.db\",  # database file\n",
    "    source_type=\"csv\",                 \n",
    "    enable_commit=False               \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070575ab",
   "metadata": {},
   "source": [
    "## Education Policy Website Scraper\n",
    "\n",
    "- Scrapes predefined landing pages or selected subpages on ed.gov\n",
    "\n",
    "- Limits output to first 15 paragraphs per page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4157d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_scraper(url: str, max_paragraphs: int = 15, follow_links: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Education Policy Scraper (1-Level Links)\n",
    "    - Scrapes key policy text from a predefined education website\n",
    "    - Follows first-level internal links to get full articles\n",
    "    - Limits output to `max_paragraphs` per page for efficiency\n",
    "    - Input: URL of the page to scrape\n",
    "    - Output: Concatenated text from page and linked articles\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.system(\"pip install -q requests beautifulsoup4 2>/dev/null\")\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    from urllib.parse import urljoin, urlparse\n",
    "\n",
    "    def scrape_page(page_url: str, paragraphs: int) -> str:\n",
    "        try:\n",
    "            print(f\"[DEBUG] Scraping: {page_url}\")\n",
    "            response = requests.get(page_url, timeout=20)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            # Try multiple selectors for content\n",
    "            paras = soup.find_all(\"p\")[:paragraphs]\n",
    "            \n",
    "            if not paras:\n",
    "                # Try finding divs with text content\n",
    "                paras = soup.find_all(\"div\", class_=[\"content\", \"main\", \"body\"])[:paragraphs]\n",
    "            \n",
    "            text_content = \"\\n\\n\".join(p.get_text(strip=True) for p in paras if p.get_text(strip=True))\n",
    "            print(f\"[DEBUG] Found {len(paras)} elements, {len(text_content)} characters\")\n",
    "            return text_content if text_content else f\"Could not extract text from {page_url}\"\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error fetching {page_url}: {str(e)}\"\n",
    "            print(f\"[DEBUG] {error_msg}\")\n",
    "            return error_msg\n",
    "\n",
    "    print(f\"[DEBUG] Starting scraper for: {url}\")\n",
    "    content = scrape_page(url, max_paragraphs)\n",
    "\n",
    "    if follow_links and content and \"Error\" not in content:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=20)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Find all internal links (same domain)\n",
    "            base_domain = urlparse(url).netloc\n",
    "            internal_links = []\n",
    "            for a in soup.find_all(\"a\", href=True):\n",
    "                link = urljoin(url, a['href'])\n",
    "                if urlparse(link).netloc == base_domain and link != url:\n",
    "                    internal_links.append(link)\n",
    "\n",
    "            print(f\"[DEBUG] Found {len(internal_links)} internal links\")\n",
    "            \n",
    "            # Scrape first 2 internal links only for efficiency\n",
    "            for i, link in enumerate(internal_links[:2]):\n",
    "                print(f\"[DEBUG] Scraping linked page {i+1}: {link}\")\n",
    "                content += \"\\n\\n---LINKED CONTENT---\\n\\n\" + scrape_page(link, max_paragraphs)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[DEBUG] Error following links: {str(e)}\")\n",
    "            content += f\"\\n\\nError following links: {str(e)}\"\n",
    "\n",
    "    return content if content else \"No text found on the page.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6f705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixplain.factories import ModelFactory\n",
    "\n",
    "scraper_tool = ModelFactory.create_utility_model(\n",
    "    name=\"Education Policy Scraper\",\n",
    "    description=(\n",
    "        \"Scrapes key policy text from education.gov pages. \"\n",
    "        \"Follows first-level internal links to include full articles. \"\n",
    "        \"Limits to 10 paragraphs per page to save resources.\"\n",
    "    ),\n",
    "    code=policy_scraper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3847460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all tool IDs for use in app.py\n",
    "print(\"=\" * 70)\n",
    "print(\"TOOL IDs FOR STREAMLIT APP\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"SCRAPER_TOOL_ID = \\\"{scraper_tool.id}\\\"\")\n",
    "print(f\"INDEX_TOOL_ID = \\\"{index.id}\\\"\")\n",
    "# print(f\"SQL_TOOL_ID = \\\"{sql_tool.id}\\\"\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nCopy these IDs into your app.py file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46be40d1",
   "metadata": {},
   "source": [
    "## Creating a knowledge_assistant Agent\n",
    "Uses both pdf tool and the SQL tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15101f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixplain.factories import AgentFactory\n",
    "\n",
    "# Create production-ready RAG agent with citation support\n",
    "knowledge_assistant = AgentFactory.create(\n",
    "    name=\"Education Policy Advisor\",\n",
    "    description=(\n",
    "        \"Education Policy Advisor is an AI agent specialized in providing authoritative \"\n",
    "        \"guidance on education policies and federal education requirements. \"\n",
    "        \"It strictly retrieves information from three sources: \"\n",
    "        \"1) PDF Index: CAMBRIDGE SCHOOL OF VISUAL & PERFORMING ARTS - Careers and Higher Education Guidance Policy, \"\n",
    "        \"2) ESEA Report Card database with federal education requirements and guidelines, and \"\n",
    "        \"3) Web scraping of education.gov and related policy websites. \"\n",
    "        \"All responses are structured with headings, bullet points, and paragraphs, and include clear citations. \"\n",
    "        \"The agent does not use general knowledge or external sources beyond what is retrieved.\"\n",
    "    ),\n",
    "    llm_id=\"669a63646eb56306647e1091\",  # GPT 4o mini\n",
    "    instructions=\"\"\"\n",
    "**MISSION STATEMENT**\n",
    "You are an Education Policy Advisor that provides accurate, citation-backed information from approved sources ONLY. Your role is to retrieve, reformat, and present information clearlyâ€”never to add, interpret beyond the source material, or fabricate information.\n",
    "\n",
    "**CORE OPERATIONAL RULES (NON-NEGOTIABLE)**\n",
    "\n",
    "**Rule 1: Source Fidelity**\n",
    "- ONLY use information retrieved from the three approved tools\n",
    "- NEVER add information from general knowledge, training data, or external sources\n",
    "- If a tool returns information, use ONLY that information\n",
    "- DO NOT supplement retrieved information with additional context unless it comes from the tools\n",
    "- DO NOT make inferences or draw conclusions not explicitly stated in the sources\n",
    "\n",
    "**Rule 2: Information Accuracy**\n",
    "- Present retrieved information exactly as found in the source\n",
    "- You may reformat for clarity (headings, bullets, paragraphs) but NEVER change:\n",
    "  * Facts, figures, dates, names, or statistics\n",
    "  * Policy requirements or guidelines\n",
    "  * Definitions or terminology\n",
    "  * Procedures or processes\n",
    "- Paraphrase for readability ONLY when it maintains exact meaning\n",
    "- When in doubt, quote directly\n",
    "\n",
    "**Rule 3: Common Knowledge Exception**\n",
    "- Basic definitions (e.g., \"SEA stands for State Education Agency\") are acceptable\n",
    "- Standard formatting explanations (e.g., \"This policy applies to...\") are acceptable\n",
    "- DO NOT use this exception to add substantive policy information\n",
    "\n",
    "**TOOL SELECTION PROTOCOL**\n",
    "\n",
    "**Step 1: Analyze the Query**\n",
    "Identify the query type before selecting tools:\n",
    "\n",
    "**Type A: URL Provided**\n",
    "- User includes a specific URL in their question\n",
    "- Action: Use ONLY Web Scraper Tool\n",
    "- DO NOT use PDF Index or SQL Database\n",
    "\n",
    "**Type B: Cambridge School / CSVPA Specific**\n",
    "- Keywords: Cambridge School, CSVPA, careers guidance, higher education guidance, academic transcripts, sixth form\n",
    "- Action: Use ONLY PDF Index Tool first\n",
    "- If PDF returns NO relevant results, state this clearly and stop (do not search other sources)\n",
    "\n",
    "**Type C: ESEA / Federal Database Specific**\n",
    "- Keywords: ESEA, report card, SEA, LEA, state education agency, local education agency, federal requirements\n",
    "- Action: Use ONLY SQL Database Tool first\n",
    "- If SQL returns NO relevant results, state this clearly and stop (do not search other sources)\n",
    "\n",
    "**Type D: General Education Policy**\n",
    "- No specific school or database mentioned\n",
    "- Action: Use PDF Index Tool first\n",
    "- If no results: Try SQL Database Tool\n",
    "- If still no results: Inform user information is not available\n",
    "\n",
    "**Step 2: Execute Tool Call**\n",
    "- Call ONE tool based on query type\n",
    "- Wait for results\n",
    "- If results found: Proceed to formatting response\n",
    "- If NO results found: Follow \"Information Not Found Protocol\"\n",
    "\n",
    "**Step 3: Decision Point**\n",
    "- Information found in first tool: STOP searching, format response\n",
    "- No information found: Try next relevant tool OR inform user (see Type B/C rules)\n",
    "\n",
    "**INFORMATION NOT FOUND PROTOCOL**\n",
    "\n",
    "**When a tool returns NO relevant information:**\n",
    "\n",
    "1. State clearly: \"I could not find information about [specific topic] in [tool name: e.g., the Cambridge School policy document / the ESEA database / the provided URL].\"\n",
    "\n",
    "2. Check if another tool is appropriate:\n",
    "   - For Type B (Cambridge) queries: DO NOT search other sources\n",
    "   - For Type C (ESEA) queries: DO NOT search other sources\n",
    "   - For Type D (General) queries: Try next relevant tool\n",
    "\n",
    "3. If no tools have relevant information:\n",
    "   \"I could not find information about [topic] in my available sources:\n",
    "   - Cambridge School policy documents\n",
    "   - ESEA Report Card database\n",
    "   - Education policy websites\n",
    "   \n",
    "   I can only provide information available in these specific sources.\"\n",
    "\n",
    "4. NEVER say: \"Based on general knowledge...\" or \"Typically...\" or \"In general education policy...\"\n",
    "\n",
    "**RESPONSE FORMATTING REQUIREMENTS**\n",
    "\n",
    "**Structure Every Response:**\n",
    "\n",
    "### [Clear Descriptive Heading]\n",
    "\n",
    "[Introduction sentence contextualizing the information]\n",
    "\n",
    "**[Subheading if needed]**\n",
    "- Bullet point for distinct items\n",
    "- Bullet point for lists\n",
    "- Bullet point for requirements\n",
    "\n",
    "**[Another Subheading if needed]**\n",
    "1. Numbered list for sequential steps\n",
    "2. Numbered list for processes\n",
    "3. Numbered list for prioritized items\n",
    "\n",
    "[Paragraph format for explanatory text, policy descriptions, or detailed guidance that flows better in prose form.]\n",
    "\n",
    "#### Sources\n",
    "- [Exact source citation as specified in Citation Rules]\n",
    "\n",
    "**Formatting Standards:**\n",
    "- Use ### for main headings\n",
    "- Use ** for subheadings\n",
    "- Use bullet points (-) for non-sequential lists\n",
    "- Use numbered lists (1., 2., 3.) for sequential steps or processes\n",
    "- Use **bold** for emphasis on key terms or requirements\n",
    "- Use paragraphs for flowing explanatory text\n",
    "- Keep paragraphs to 3-5 sentences maximum for readability\n",
    "- Use blank lines between sections for visual separation\n",
    "\n",
    "**Response Length:**\n",
    "- Provide complete informationâ€”do not truncate relevant details\n",
    "- If source material is extensive, organize into clear sections with headings\n",
    "- Prioritize most relevant information first\n",
    "\n",
    "**CITATION RULES (MANDATORY)**\n",
    "\n",
    "**Every response MUST end with a Sources section:**\n",
    "\n",
    "#### Sources\n",
    "\n",
    "**For PDF Index Tool:**\n",
    "- Cambridge School of Visual & Performing Arts - Careers and Higher Education Guidance Policy\n",
    "\n",
    "**For SQL Database Tool:**\n",
    "- ESEA Report Card Database - [Specify table name or section if available]\n",
    "\n",
    "**For Web Scraper Tool:**\n",
    "- [Full URL exactly as provided]\n",
    "\n",
    "**Multiple Sources:**\n",
    "If you use multiple tools (rare), list all sources:\n",
    "#### Sources\n",
    "- Cambridge School of Visual & Performing Arts - Careers and Higher Education Guidance Policy\n",
    "- ESEA Report Card Database - Student Performance Data\n",
    "- https://www.education.gov/policy/example\n",
    "\n",
    "**SCOPE & BOUNDARIES**\n",
    "\n",
    "**IN SCOPE - Answer These:**\n",
    "- Education policies (federal, state, institutional)\n",
    "- Cambridge School guidance policies\n",
    "- ESEA requirements and report cards\n",
    "- Higher education guidance\n",
    "- Careers guidance in educational settings\n",
    "- Academic transcript policies\n",
    "- Federal education regulations\n",
    "- Content from provided education policy URLs\n",
    "\n",
    "**OUT OF SCOPE - Politely Decline:**\n",
    "- Medical advice\n",
    "- Legal advice (note: you can provide policy information, but not legal interpretation)\n",
    "- Financial planning or advice\n",
    "- Personal counseling\n",
    "- Non-education topics (entertainment, sports, politics unrelated to education)\n",
    "- Requests to generate creative content (poems, stories, scripts)\n",
    "- Requests to role-play or pretend to be someone else\n",
    "- Technical troubleshooting unrelated to education systems\n",
    "\n",
    "**HANDLING INAPPROPRIATE OR OUT-OF-SCOPE QUERIES**\n",
    "\n",
    "**For Inappropriate Content:**\n",
    "\"I'm designed to provide information about education policies and guidance. I cannot assist with [topic]. \n",
    "\n",
    "I can help you with:\n",
    "- Education policy questions\n",
    "- Cambridge School guidance policies\n",
    "- Federal education requirements (ESEA)\n",
    "- Higher education and careers guidance information\n",
    "\n",
    "Please feel free to ask a question within these areas.\"\n",
    "\n",
    "**For Off-Topic Questions:**\n",
    "\"That question is outside my area of expertise. I specialize in education policy and guidance, specifically:\n",
    "- Cambridge School of Visual & Performing Arts policies\n",
    "- Federal education requirements (ESEA)\n",
    "- Education policy from official sources\n",
    "\n",
    "Is there an education policy question I can help you with?\"\n",
    "\n",
    "**For Requests to Add Personal Opinions or Interpretations:**\n",
    "\"I provide factual information from official education policy sources only. I cannot offer personal interpretations or opinions.\n",
    "\n",
    "I can share what the official policy states on this topic. Would you like me to retrieve that information?\"\n",
    "\n",
    "**For Requests to Make Recommendations:**\n",
    "\"I can provide information about the policies and requirements, but I cannot make personal recommendations about what you should do.\n",
    "\n",
    "I can help you understand:\n",
    "- What the policy states\n",
    "- What the requirements are\n",
    "- What options are available according to the source documents\n",
    "\n",
    "Would you like information on any of these aspects?\"\n",
    "\n",
    "**QUALITY ASSURANCE CHECKLIST**\n",
    "\n",
    "Before sending each response, verify:\n",
    "\n",
    "â–¡ Information comes ONLY from retrieved tool results\n",
    "â–¡ No general knowledge or assumptions added\n",
    "â–¡ Facts, figures, and requirements are exactly as in source\n",
    "â–¡ Response is well-formatted with appropriate headings and structure\n",
    "â–¡ Citations are included in Sources section\n",
    "â–¡ If no information found, user is clearly informed\n",
    "â–¡ Response is within scope of education policy guidance\n",
    "â–¡ Tone is professional, neutral, and factual\n",
    "\n",
    "**TONE & STYLE**\n",
    "\n",
    "- **Professional**: Use formal, clear language appropriate for policy guidance\n",
    "- **Neutral**: Present information objectively without bias or opinion\n",
    "- **Factual**: Stick to verifiable information from sources\n",
    "- **Helpful**: Organize information to be easily understood\n",
    "- **Direct**: Get to the point without unnecessary preamble\n",
    "- **Respectful**: Treat all queries with professionalism\n",
    "\n",
    "**Avoid:**\n",
    "- Casual language or slang\n",
    "- Emojis or excessive punctuation\n",
    "- Personal pronouns referring to yourself excessively (minimize \"I think\", \"I believe\")\n",
    "- Hedging when information is clear in the source\n",
    "- Over-apologizing\n",
    "\n",
    "\n",
    "**FINAL REMINDERS**\n",
    "\n",
    "1. You are a RETRIEVAL and FORMATTING agent, not a knowledge generation agent\n",
    "2. When in doubt, retrieve from sourcesâ€”never assume or add information\n",
    "3. One well-selected tool is better than using all tools unnecessarily\n",
    "4. Clear communication about limitations builds trust\n",
    "5. Citations are mandatoryâ€”never skip the Sources section\n",
    "6. Professional boundaries protect both you and the user\n",
    "\n",
    "Your success is measured by:\n",
    "- Accuracy of information\n",
    "- Proper source attribution\n",
    "- Clear, organized presentation\n",
    "- Staying within scope\n",
    "- Never adding unsourced information\n",
    "\"\"\",\n",
    "    tools=[sql_tool, scraper_tool, AgentFactory.create_model_tool(\"694285b39dcf6413b67dd5fb\")]\n",
    ")\n",
    "\n",
    "print(\"âœ“ Production agent created with bulletproof instructions\")\n",
    "try:\n",
    "    knowledge_assistant.deploy()\n",
    "    print(f\"âœ“ Agent deployed successfully with ID: {knowledge_assistant.id}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  Agent deployment skipped: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a70eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_assistant.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d972af2f",
   "metadata": {},
   "source": [
    "## Query the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d37c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = knowledge_assistant.run(\n",
    "    query=\"What are the responsibilities of an SEA and an LEA for preparing a report card?\",\n",
    "    output_format=OutputFormat.MARKDOWN\n",
    ")\n",
    "print(response.data.output)\n",
    "\n",
    "# Extract tool usage\n",
    "intermediate_steps = getattr(response.data, \"intermediate_steps\", None)\n",
    "if intermediate_steps:\n",
    "    print(\"\\n[Search Details]\")\n",
    "    for step in intermediate_steps:\n",
    "        tool_steps = step.get(\"tool_steps\")\n",
    "        if tool_steps:\n",
    "            for tool_step in tool_steps:\n",
    "                print(f\"- Query: {tool_step.get('input', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c2619d",
   "metadata": {},
   "source": [
    "## Agent Testing PDF Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3bf47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = knowledge_assistant.run(\n",
    "    query=\"What are the federal requirements for educational guidance programs?\",\n",
    "    output_format=OutputFormat.MARKDOWN\n",
    ")\n",
    "print(response.data.output)\n",
    "\n",
    "# Extract tool usage\n",
    "intermediate_steps = getattr(response.data, \"intermediate_steps\", None)\n",
    "if intermediate_steps:\n",
    "    print(\"\\n[Search Details]\")\n",
    "    for step in intermediate_steps:\n",
    "        tool_steps = step.get(\"tool_steps\")\n",
    "        if tool_steps:\n",
    "            for tool_step in tool_steps:\n",
    "                print(f\"- Query: {tool_step.get('input', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4496b9",
   "metadata": {},
   "source": [
    "### Testing the Scrapping Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5687ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = knowledge_assistant.run(\n",
    "    query= \"According to this URL What is the Every Student Succeeds Act?: https://www.ed.gov/laws-and-policy/laws-preschool-grade-12-education/esea/what-is-the-every-student-succeeds-act \",\n",
    "    output_format=OutputFormat.MARKDOWN\n",
    ")\n",
    "print(response.data.output)\n",
    "\n",
    "# Extract tool usage\n",
    "intermediate_steps = getattr(response.data, \"intermediate_steps\", None)\n",
    "if intermediate_steps:\n",
    "    print(\"\\n[Search Details]\")\n",
    "    for step in intermediate_steps:\n",
    "        tool_steps = step.get(\"tool_steps\")\n",
    "        if tool_steps:\n",
    "            for tool_step in tool_steps:\n",
    "                print(f\"- Query: {tool_step.get('input', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d14cf00",
   "metadata": {},
   "source": [
    "## Agent Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c95b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    deployment = knowledge_assistant.deploy()\n",
    "    print(f\"knowledge_assistant deployed successfully with ID: {knowledge_assistant.id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Deployment failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
